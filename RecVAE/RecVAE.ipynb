{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQG9hL8-UQUP"
   },
   "source": [
    "# RecVAE\n",
    "üìöReferences code  \n",
    "https://github.com/ilya-shenbin/RecVAE  \n",
    "https://github.com/younggyoseo/vae-cf-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7CfnRw7U59C"
   },
   "source": [
    "## Ï¥àÍ∏∞ ÏÑ∏ÌåÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20039,
     "status": "ok",
     "timestamp": 1647338351940,
     "user": {
      "displayName": "Jungwon Seo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-5uwnfogZleKrsyZupiP0LpS_PBPloa_pqKvqTg=s64",
      "userId": "00987919963196484028"
     },
     "user_tz": -540
    },
    "id": "EWWEf1mPKdnX",
    "outputId": "c2ab2ddf-9a02-4bc8-8133-ff5c95f9e366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.0.1 in /opt/conda/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.0.1) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas==1.0.1) (2020.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.8/site-packages (from pandas==1.0.1) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas==1.0.1) (1.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install pandas==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "bQj6k1mSbxaz"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1647338362900,
     "user": {
      "displayName": "Jungwon Seo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-5uwnfogZleKrsyZupiP0LpS_PBPloa_pqKvqTg=s64",
      "userId": "00987919963196484028"
     },
     "user_tz": -540
    },
    "id": "xQ3W0udmbxa3",
    "outputId": "a711d6bb-0f42-451f-d97b-7fed34682010"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Í∞ÅÏ¢Ö ÌååÎùºÎØ∏ÌÑ∞ ÏÑ∏ÌåÖ\n",
    "parser = argparse.ArgumentParser(description='PyTorch RecVAE')\n",
    "\n",
    "\n",
    "parser.add_argument('--data', type=str, default='/opt/ml/input/data/train/',\n",
    "                    help='Movielens dataset location')\n",
    "\n",
    "parser.add_argument('--dataset', type=str)\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--batch-size', type=int, default=500)\n",
    "parser.add_argument('--beta', type=float, default=None)\n",
    "parser.add_argument('--gamma', type=float, default=0.005)\n",
    "parser.add_argument('--lr', type=float, default=5e-4)\n",
    "parser.add_argument('--n-epochs', type=int, default=50)\n",
    "parser.add_argument('--n-enc_epochs', type=int, default=3)\n",
    "parser.add_argument('--n-dec_epochs', type=int, default=1)\n",
    "parser.add_argument('--not-alternating', type=bool, default=False)\n",
    "\n",
    "parser.add_argument('--save', type=str, default='model.pt',\n",
    "                    help='path to save the final model')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# Set the random seed manually for reproductibility.\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "#ÎßåÏïΩ GPUÍ∞Ä ÏÇ¨Ïö©Í∞ÄÎä•Ìïú ÌôòÍ≤ΩÏù¥ÎùºÎ©¥ GPUÎ•º ÏÇ¨Ïö©\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7o1fvXqFWE_G"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "cgvNoy1Ybxa6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "\n",
    "    return count\n",
    "\n",
    "# ÌäπÏ†ïÌïú ÌöüÏàò Ïù¥ÏÉÅÏùò Î¶¨Î∑∞Í∞Ä Ï°¥Ïû¨ÌïòÎäî(ÏÇ¨Ïö©ÏûêÏùò Í≤ΩÏö∞ min_uc Ïù¥ÏÉÅ, ÏïÑÏù¥ÌÖúÏùò Í≤ΩÏö∞ min_scÏù¥ÏÉÅ) \n",
    "# Îç∞Ïù¥ÌÑ∞ÎßåÏùÑ Ï∂îÏ∂úÌï† Îïå ÏÇ¨Ïö©ÌïòÎäî Ìï®ÏàòÏûÖÎãàÎã§.\n",
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'item')\n",
    "        tp = tp[tp['item'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'user')\n",
    "        tp = tp[tp['user'].isin(usercount.index[usercount >= min_uc])]\n",
    "\n",
    "    usercount, itemcount = get_count(tp, 'user'), get_count(tp, 'item')\n",
    "    return tp, usercount, itemcount\n",
    "\n",
    "#ÌõàÎ†®Îêú Î™®Îç∏ÏùÑ Ïù¥Ïö©Ìï¥ Í≤ÄÏ¶ùÌï† Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÎ¶¨ÌïòÎäî Ìï®ÏàòÏûÖÎãàÎã§.\n",
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('user')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "    \n",
    "    for _, group in data_grouped_by_user:\n",
    "        n_items_u = len(group)\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        \n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "    \n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    return data_tr, data_te\n",
    "\n",
    "def numerize(tp, profile2id, show2id):\n",
    "    uid = tp['user'].apply(lambda x: profile2id[x])\n",
    "    sid = tp['item'].apply(lambda x: show2id[x])\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2812,
     "status": "ok",
     "timestamp": 1647338365706,
     "user": {
      "displayName": "Jungwon Seo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-5uwnfogZleKrsyZupiP0LpS_PBPloa_pqKvqTg=s64",
      "userId": "00987919963196484028"
     },
     "user_tz": -540
    },
    "id": "fVFoRHrmVQsp",
    "outputId": "d742d219-96a0-4334-afff-685a368ec622"
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "DATA_DIR = args.data\n",
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'train_ratings.csv'), header=0)\n",
    "\n",
    "# Filter Data\n",
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data, min_uc=5, min_sc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1647338365706,
     "user": {
      "displayName": "Jungwon Seo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-5uwnfogZleKrsyZupiP0LpS_PBPloa_pqKvqTg=s64",
      "userId": "00987919963196484028"
     },
     "user_tz": -540
    },
    "id": "7T1dTsWUrffP",
    "outputId": "68476060-3cf6-473e-f21c-05e9cda5bdb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(BEFORE) unique_uid: Int64Index([    11,     14,     18,     25,     31,     35,     43,     50,\n",
      "                58,     60,\n",
      "            ...\n",
      "            138459, 138461, 138470, 138471, 138472, 138473, 138475, 138486,\n",
      "            138492, 138493],\n",
      "           dtype='int64', name='user', length=31360)\n",
      "(AFTER) unique_uid: Int64Index([ 27968,  67764,   2581,  82969, 137831,  48639,  97870,  40424,\n",
      "             46835,  79570,\n",
      "            ...\n",
      "            114284,   9009,  21165,  33920,  22054, 135379, 125855,  41891,\n",
      "             15720,  17029],\n",
      "           dtype='int64', name='user', length=31360)\n",
      "ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïóê ÏÇ¨Ïö©Îê† ÏÇ¨Ïö©Ïûê Ïàò: 31360\n",
      "Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Ïóê ÏÇ¨Ïö©Îê† ÏÇ¨Ïö©Ïûê Ïàò: 3000\n",
      "ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Ïóê ÏÇ¨Ïö©Îê† ÏÇ¨Ïö©Ïûê Ïàò: 3000\n"
     ]
    }
   ],
   "source": [
    "# Shuffle User Indices\n",
    "unique_uid = user_activity.index\n",
    "all_users = unique_uid\n",
    "print(\"(BEFORE) unique_uid:\",unique_uid)\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]\n",
    "print(\"(AFTER) unique_uid:\",unique_uid)\n",
    "\n",
    "n_users = unique_uid.size #31360\n",
    "n_heldout_users = 3000\n",
    "\n",
    "\n",
    "# Split Train/Validation/Test User Indices\n",
    "#tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "tr_users = unique_uid\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]\n",
    "\n",
    "print(\"ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïóê ÏÇ¨Ïö©Îê† ÏÇ¨Ïö©Ïûê Ïàò:\", len(tr_users))\n",
    "print(\"Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Ïóê ÏÇ¨Ïö©Îê† ÏÇ¨Ïö©Ïûê Ïàò:\", len(vd_users))\n",
    "print(\"ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Ïóê ÏÇ¨Ïö©Îê† ÏÇ¨Ïö©Ïûê Ïàò:\", len(te_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30590,
     "status": "ok",
     "timestamp": 1647338396290,
     "user": {
      "displayName": "Jungwon Seo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-5uwnfogZleKrsyZupiP0LpS_PBPloa_pqKvqTg=s64",
      "userId": "00987919963196484028"
     },
     "user_tz": -540
    },
    "id": "3yBsRCRqtPz6",
    "outputId": "c1dcf49a-a33d-482e-f321-db4fb2cacb1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "##ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïóê Ìï¥ÎãπÌïòÎäî ÏïÑÏù¥ÌÖúÎì§\n",
    "train_plays = raw_data.loc[raw_data['user'].isin(tr_users)]\n",
    "\n",
    "##ÏïÑÏù¥ÌÖú ID\n",
    "unique_sid = pd.unique(train_plays['item'])\n",
    "\n",
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
    "\n",
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "\n",
    "if not os.path.exists(pro_dir):\n",
    "    os.makedirs(pro_dir)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)\n",
    "\n",
    "#ValidationÍ≥º TestÏóêÎäî inputÏúºÎ°ú ÏÇ¨Ïö©Îê† tr Îç∞Ïù¥ÌÑ∞ÏôÄ Ï†ïÎãµÏùÑ ÌôïÏù∏ÌïòÍ∏∞ ÏúÑÌïú te Îç∞Ïù¥ÌÑ∞Î°ú Î∂ÑÎ¶¨ÎêòÏóàÏäµÎãàÎã§.\n",
    "vad_plays = raw_data.loc[raw_data['user'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['item'].isin(unique_sid)]\n",
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
    "\n",
    "test_plays = raw_data.loc[raw_data['user'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['item'].isin(unique_sid)]\n",
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
    "\n",
    "train_data = numerize(train_plays, profile2id, show2id)\n",
    "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
    "\n",
    "vad_data_tr = numerize(vad_plays_tr, profile2id, show2id)\n",
    "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
    "\n",
    "vad_data_te = numerize(vad_plays_te, profile2id, show2id)\n",
    "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
    "\n",
    "test_data_tr = numerize(test_plays_tr, profile2id, show2id)\n",
    "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
    "\n",
    "test_data_te = numerize(test_plays_te, profile2id, show2id)\n",
    "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           uid   sid\n",
      "0            0     0\n",
      "1            0     1\n",
      "2            0     2\n",
      "3            0     3\n",
      "4            0     4\n",
      "...        ...   ...\n",
      "5154466  31359   423\n",
      "5154467  31359  1491\n",
      "5154468  31359   331\n",
      "5154469  31359   733\n",
      "5154470  31359  2256\n",
      "\n",
      "[5154471 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#submissionÏóê ÏÇ¨Ïö©Ìï† Îç∞Ïù¥ÌÑ∞\n",
    "submission_plays = raw_data.loc[raw_data['user'].isin(all_users)]\n",
    "\n",
    "##ÏïÑÏù¥ÌÖú ID\n",
    "unique_sid = pd.unique(submission_plays['item'])\n",
    "\n",
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(all_users))\n",
    "\n",
    "sub_data = numerize(submission_plays, profile2id, show2id)\n",
    "sub_data.to_csv(os.path.join(pro_dir, 'sub.csv'), index=False)\n",
    "print(sub_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMiq9leyWWL1"
   },
   "source": [
    "## Îç∞Ïù¥ÌÑ∞ Î°úÎçî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "nxUADr9ibxa8"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DataLoader():\n",
    "    '''\n",
    "    Load Movielens dataset\n",
    "    '''\n",
    "    def __init__(self, path):\n",
    "        \n",
    "        self.pro_dir = os.path.join(path, 'pro_sg')\n",
    "        assert os.path.exists(self.pro_dir), \"Preprocessed files do not exist. Run data.py\"\n",
    "\n",
    "        self.n_items = self.load_n_items()\n",
    "    \n",
    "    def load_data(self, datatype='train'):\n",
    "        if datatype == 'train':\n",
    "            return self._load_train_data()\n",
    "        elif datatype == 'validation':\n",
    "            return self._load_tr_te_data(datatype)\n",
    "        elif datatype == 'test':\n",
    "            return self._load_tr_te_data(datatype)\n",
    "        elif datatype == 'sub':\n",
    "            return self._load_submission_data()\n",
    "        else:\n",
    "            raise ValueError(\"datatype should be in [train, validation, test, sub]\")\n",
    "        \n",
    "    def load_n_items(self):\n",
    "        unique_sid = list()\n",
    "        with open(os.path.join(self.pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                unique_sid.append(line.strip())\n",
    "        n_items = len(unique_sid)\n",
    "        return n_items\n",
    "    \n",
    "    def _load_submission_data(self):\n",
    "        path = os.path.join(self.pro_dir, 'sub.csv')\n",
    "        \n",
    "        tp = pd.read_csv(path)\n",
    "        n_users = tp['uid'].max() + 1\n",
    "        rows, cols = tp['uid'], tp['sid']\n",
    "        data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                                 (rows, cols)), dtype='float64',\n",
    "                                 shape=(n_users, self.n_items))\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _load_train_data(self):\n",
    "        path = os.path.join(self.pro_dir, 'train.csv')\n",
    "        \n",
    "        tp = pd.read_csv(path)\n",
    "        n_users = tp['uid'].max() + 1\n",
    "        rows, cols = tp['uid'], tp['sid']\n",
    "        data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                                 (rows, cols)), dtype='float64',\n",
    "                                 shape=(n_users, self.n_items))\n",
    "        return data\n",
    "    \n",
    "    def _load_tr_te_data(self, datatype='test'):\n",
    "        tr_path = os.path.join(self.pro_dir, '{}_tr.csv'.format(datatype))\n",
    "        te_path = os.path.join(self.pro_dir, '{}_te.csv'.format(datatype))\n",
    "\n",
    "        tp_tr = pd.read_csv(tr_path)\n",
    "        tp_te = pd.read_csv(te_path)\n",
    "\n",
    "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "        rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "        rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "        data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                                    (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
    "        data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                                    (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
    "        return data_tr, data_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RecVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "QYlGPJTYU0ii"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x.mul(torch.sigmoid(x))\n",
    "\n",
    "def log_norm_pdf(x, mu, logvar):\n",
    "    return -0.5*(logvar + np.log(2 * np.pi) + (x - mu).pow(2) / logvar.exp())\n",
    "\n",
    "\n",
    "class CompositePrior(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim, mixture_weights=[3/20, 3/4, 1/10]):\n",
    "        super(CompositePrior, self).__init__()\n",
    "        \n",
    "        self.mixture_weights = mixture_weights\n",
    "        \n",
    "        self.mu_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.mu_prior.data.fill_(0)\n",
    "        \n",
    "        self.logvar_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.logvar_prior.data.fill_(0)\n",
    "        \n",
    "        self.logvar_uniform_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.logvar_uniform_prior.data.fill_(10)\n",
    "        \n",
    "        self.encoder_old = Encoder(hidden_dim, latent_dim, input_dim)\n",
    "        self.encoder_old.requires_grad_(False)\n",
    "        \n",
    "    def forward(self, x, z):\n",
    "        post_mu, post_logvar = self.encoder_old(x, 0)\n",
    "        \n",
    "        stnd_prior = log_norm_pdf(z, self.mu_prior, self.logvar_prior)\n",
    "        post_prior = log_norm_pdf(z, post_mu, post_logvar)\n",
    "        unif_prior = log_norm_pdf(z, self.mu_prior, self.logvar_uniform_prior)\n",
    "        \n",
    "        gaussians = [stnd_prior, post_prior, unif_prior]\n",
    "        gaussians = [g.add(np.log(w)) for g, w in zip(gaussians, self.mixture_weights)]\n",
    "        \n",
    "        density_per_gaussian = torch.stack(gaussians, dim=-1)\n",
    "                \n",
    "        return torch.logsumexp(density_per_gaussian, dim=-1)\n",
    "\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln3 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln4 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln5 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x, dropout_rate):\n",
    "        norm = x.pow(2).sum(dim=-1).sqrt()\n",
    "        x = x / norm[:, None]\n",
    "    \n",
    "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
    "        \n",
    "        h1 = self.ln1(swish(self.fc1(x)))\n",
    "        h2 = self.ln2(swish(self.fc2(h1) + h1))\n",
    "        h3 = self.ln3(swish(self.fc3(h2) + h1 + h2))\n",
    "        h4 = self.ln4(swish(self.fc4(h3) + h1 + h2 + h3))\n",
    "        h5 = self.ln5(swish(self.fc5(h4) + h1 + h2 + h3 + h4))\n",
    "        return self.fc_mu(h5), self.fc_logvar(h5)\n",
    "    \n",
    "\n",
    "class RecVAE(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim):\n",
    "        super(RecVAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(hidden_dim, latent_dim, input_dim)\n",
    "        self.prior = CompositePrior(hidden_dim, latent_dim, input_dim)\n",
    "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, user_ratings, beta=None, gamma=1, dropout_rate=0.5, calculate_loss=True):\n",
    "        mu, logvar = self.encoder(user_ratings, dropout_rate=dropout_rate)    \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_pred = self.decoder(z)\n",
    "        \n",
    "        if calculate_loss:\n",
    "            if gamma:\n",
    "                norm = user_ratings.sum(dim=-1)\n",
    "                kl_weight = gamma * norm\n",
    "            elif beta:\n",
    "                kl_weight = beta\n",
    "\n",
    "            mll = (F.log_softmax(x_pred, dim=-1) * user_ratings).sum(dim=-1).mean()\n",
    "            kld = (log_norm_pdf(z, mu, logvar) - self.prior(user_ratings, z)).sum(dim=-1).mul(kl_weight).mean()\n",
    "            negative_elbo = -(mll - kld)\n",
    "            \n",
    "            return (mll, kld), negative_elbo\n",
    "            \n",
    "        else:\n",
    "            return x_pred\n",
    "\n",
    "    def update_prior(self):\n",
    "        self.prior.encoder_old.load_state_dict(deepcopy(self.encoder.state_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, device, data_in, data_out=None, shuffle=False, samples_perc_per_epoch=1):\n",
    "    assert 0 < samples_perc_per_epoch <= 1\n",
    "    \n",
    "    total_samples = data_in.shape[0]\n",
    "    samples_per_epoch = int(total_samples * samples_perc_per_epoch)\n",
    "    \n",
    "    if shuffle:\n",
    "        idxlist = np.arange(total_samples)\n",
    "        np.random.shuffle(idxlist)\n",
    "        idxlist = idxlist[:samples_per_epoch]\n",
    "    else:\n",
    "        idxlist = np.arange(samples_per_epoch)\n",
    "    \n",
    "    for st_idx in range(0, samples_per_epoch, batch_size):\n",
    "        end_idx = min(st_idx + batch_size, samples_per_epoch)\n",
    "        idx = idxlist[st_idx:end_idx]\n",
    "\n",
    "        yield Batch(device, idx, data_in, data_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, device, idx, data_in, data_out=None):\n",
    "        self._device = device\n",
    "        self._idx = idx\n",
    "        self._data_in = data_in\n",
    "        self._data_out = data_out\n",
    "    \n",
    "    def get_idx(self):\n",
    "        return self._idx\n",
    "    \n",
    "    def get_idx_to_dev(self):\n",
    "        return torch.LongTensor(self.get_idx()).to(self._device)\n",
    "        \n",
    "    def get_ratings(self, is_out=False):\n",
    "        data = self._data_out if is_out else self._data_in\n",
    "        return data[self._idx]\n",
    "    \n",
    "    def get_ratings_to_dev(self, is_out=False):\n",
    "        return torch.Tensor(\n",
    "            self.get_ratings(is_out).toarray()\n",
    "        ).to(self._device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, opts, train_data, batch_size, n_epochs, beta, gamma, dropout_rate):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in generate(batch_size=batch_size, device=device, data_in=train_data, shuffle=True):\n",
    "            ratings = batch.get_ratings_to_dev()\n",
    "\n",
    "            for optimizer in opts:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            _, loss = model(ratings, beta=beta, gamma=gamma, dropout_rate=dropout_rate)\n",
    "            loss.backward()\n",
    "            \n",
    "            for optimizer in opts:\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "def evaluate(model, data_in, data_out, metrics, samples_perc_per_epoch=1, batch_size=500):\n",
    "    metrics = deepcopy(metrics)\n",
    "    model.eval()\n",
    "    \n",
    "    for m in metrics:\n",
    "        m['score'] = []\n",
    "    \n",
    "    for batch in generate(batch_size=batch_size,\n",
    "                          device=device,\n",
    "                          data_in=data_in,\n",
    "                          data_out=data_out,\n",
    "                          samples_perc_per_epoch=samples_perc_per_epoch\n",
    "                         ):\n",
    "        \n",
    "        ratings_in = batch.get_ratings_to_dev()\n",
    "        ratings_out = batch.get_ratings(is_out=True)\n",
    "    \n",
    "        ratings_pred = model(ratings_in, calculate_loss=False).cpu().detach().numpy()\n",
    "        if not (data_in is data_out):\n",
    "            ratings_pred[batch.get_ratings().nonzero()] = -np.inf\n",
    "            \n",
    "        for m in metrics:\n",
    "            m['score'].append(m['metric'](ratings_pred, ratings_out, k=m['k']))\n",
    "\n",
    "    for m in metrics:\n",
    "        m['score'] = np.concatenate(m['score']).mean()\n",
    "        \n",
    "    return [x['score'] for x in metrics]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOsCJbb_X9gl"
   },
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "zxNtit6vbxa-"
   },
   "outputs": [],
   "source": [
    "import bottleneck as bn\n",
    "import numpy as np\n",
    "\n",
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1QjCbMBXw4v"
   },
   "source": [
    "## RecVAE ÌÖåÏä§Ìä∏ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "78zFFNzgbxa_"
   },
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "loader = DataLoader(args.data)\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "valid_in_data, valid_out_data = loader.load_data('validation')\n",
    "test_in_data, test_out_data = loader.load_data('test')\n",
    "\n",
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "##inferenceÏóê Ïì∏Í≤É\n",
    "sub_data = loader.load_data('sub')\n",
    "\n",
    "s_N = sub_data.shape[0]\n",
    "s_idxlist = list(range(s_N))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "model = RecVAE(600, 200, n_items).to(device)\n",
    "model_best = RecVAE(600, 200, n_items).to(device)\n",
    "\n",
    "learning_kwargs = {\n",
    "    'model': model,\n",
    "    'train_data': train_data,\n",
    "    'batch_size': args.batch_size,\n",
    "    'beta': args.beta,\n",
    "    'gamma': args.gamma\n",
    "}\n",
    "\n",
    "decoder_params = set(model.decoder.parameters())\n",
    "encoder_params = set(model.encoder.parameters())\n",
    "\n",
    "optimizer_encoder = optim.Adam(encoder_params, lr=args.lr)\n",
    "optimizer_decoder = optim.Adam(decoder_params, lr=args.lr)\n",
    "\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "metrics = [{'metric': Recall_at_k_batch, 'k': 10}]\n",
    "\n",
    "train_scores, valid_scores = [], []\n",
    "\n",
    "best_r10 = -np.inf\n",
    "update_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53128,
     "status": "ok",
     "timestamp": 1647338513780,
     "user": {
      "displayName": "Jungwon Seo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-5uwnfogZleKrsyZupiP0LpS_PBPloa_pqKvqTg=s64",
      "userId": "00987919963196484028"
     },
     "user_tz": -540
    },
    "id": "WoUFwndCvvtp",
    "outputId": "5aef1fa2-a616-4494-d48a-6c8997e3190c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "epoch 1 | valid recall@10: 0.2213 | best valid: 0.2213 | train recall@10: 0.6837\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 2 | valid recall@10: 0.3309 | best valid: 0.3309 | train recall@10: 0.8204\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 3 | valid recall@10: 0.3509 | best valid: 0.3509 | train recall@10: 0.8393\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 4 | valid recall@10: 0.3622 | best valid: 0.3622 | train recall@10: 0.8425\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 5 | valid recall@10: 0.3719 | best valid: 0.3719 | train recall@10: 0.8508\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 6 | valid recall@10: 0.3817 | best valid: 0.3817 | train recall@10: 0.8559\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 7 | valid recall@10: 0.3874 | best valid: 0.3874 | train recall@10: 0.8511\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 8 | valid recall@10: 0.3908 | best valid: 0.3908 | train recall@10: 0.8546\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 9 | valid recall@10: 0.3932 | best valid: 0.3932 | train recall@10: 0.8537\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 10 | valid recall@10: 0.3949 | best valid: 0.3949 | train recall@10: 0.8572\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 11 | valid recall@10: 0.3984 | best valid: 0.3984 | train recall@10: 0.8597\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 12 | valid recall@10: 0.3996 | best valid: 0.3996 | train recall@10: 0.8581\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 13 | valid recall@10: 0.3986 | best valid: 0.3996 | train recall@10: 0.8620\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 14 | valid recall@10: 0.4018 | best valid: 0.4018 | train recall@10: 0.8575\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 15 | valid recall@10: 0.4059 | best valid: 0.4059 | train recall@10: 0.8604\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 16 | valid recall@10: 0.4067 | best valid: 0.4067 | train recall@10: 0.8629\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 17 | valid recall@10: 0.4077 | best valid: 0.4077 | train recall@10: 0.8655\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 18 | valid recall@10: 0.4093 | best valid: 0.4093 | train recall@10: 0.8636\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 19 | valid recall@10: 0.4118 | best valid: 0.4118 | train recall@10: 0.8671\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 20 | valid recall@10: 0.4116 | best valid: 0.4118 | train recall@10: 0.8681\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 21 | valid recall@10: 0.4129 | best valid: 0.4129 | train recall@10: 0.8716\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 22 | valid recall@10: 0.4148 | best valid: 0.4148 | train recall@10: 0.8709\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 23 | valid recall@10: 0.4163 | best valid: 0.4163 | train recall@10: 0.8712\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 24 | valid recall@10: 0.4179 | best valid: 0.4179 | train recall@10: 0.8757\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 25 | valid recall@10: 0.4181 | best valid: 0.4181 | train recall@10: 0.8764\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 26 | valid recall@10: 0.4198 | best valid: 0.4198 | train recall@10: 0.8757\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 27 | valid recall@10: 0.4209 | best valid: 0.4209 | train recall@10: 0.8786\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 28 | valid recall@10: 0.4192 | best valid: 0.4209 | train recall@10: 0.8786\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 29 | valid recall@10: 0.4212 | best valid: 0.4212 | train recall@10: 0.8780\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 30 | valid recall@10: 0.4223 | best valid: 0.4223 | train recall@10: 0.8824\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 31 | valid recall@10: 0.4250 | best valid: 0.4250 | train recall@10: 0.8850\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 32 | valid recall@10: 0.4233 | best valid: 0.4250 | train recall@10: 0.8847\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 33 | valid recall@10: 0.4258 | best valid: 0.4258 | train recall@10: 0.8843\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 34 | valid recall@10: 0.4259 | best valid: 0.4259 | train recall@10: 0.8872\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 35 | valid recall@10: 0.4270 | best valid: 0.4270 | train recall@10: 0.8898\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 36 | valid recall@10: 0.4280 | best valid: 0.4280 | train recall@10: 0.8875\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 37 | valid recall@10: 0.4267 | best valid: 0.4280 | train recall@10: 0.8850\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 38 | valid recall@10: 0.4280 | best valid: 0.4280 | train recall@10: 0.8888\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 39 | valid recall@10: 0.4291 | best valid: 0.4291 | train recall@10: 0.8927\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 40 | valid recall@10: 0.4295 | best valid: 0.4295 | train recall@10: 0.8863\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 41 | valid recall@10: 0.4287 | best valid: 0.4295 | train recall@10: 0.8898\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 42 | valid recall@10: 0.4302 | best valid: 0.4302 | train recall@10: 0.8904\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 43 | valid recall@10: 0.4313 | best valid: 0.4313 | train recall@10: 0.8911\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 44 | valid recall@10: 0.4333 | best valid: 0.4333 | train recall@10: 0.8907\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 45 | valid recall@10: 0.4322 | best valid: 0.4333 | train recall@10: 0.8927\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 46 | valid recall@10: 0.4326 | best valid: 0.4333 | train recall@10: 0.8920\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 47 | valid recall@10: 0.4336 | best valid: 0.4336 | train recall@10: 0.8968\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 48 | valid recall@10: 0.4329 | best valid: 0.4336 | train recall@10: 0.8936\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 49 | valid recall@10: 0.4340 | best valid: 0.4340 | train recall@10: 0.8987\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "epoch 50 | valid recall@10: 0.4345 | best valid: 0.4345 | train recall@10: 0.8930\n",
      "=========================================================================================\n",
      "=========================================================================================\n",
      "| End of training | Recall_at_k_batch@10:\t0.4335\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.n_epochs+1):\n",
    "\n",
    "    if args.not_alternating:\n",
    "        run(opts=[optimizer_encoder, optimizer_decoder], n_epochs=1, dropout_rate=0.5, **learning_kwargs)\n",
    "    else:\n",
    "        run(opts=[optimizer_encoder], n_epochs=args.n_enc_epochs, dropout_rate=0.5, **learning_kwargs)\n",
    "        model.update_prior()\n",
    "        run(opts=[optimizer_decoder], n_epochs=args.n_dec_epochs, dropout_rate=0, **learning_kwargs)\n",
    "\n",
    "    train_scores.append(\n",
    "        evaluate(model, train_data, train_data, metrics, 0.01)[0]\n",
    "    )\n",
    "    valid_scores.append(\n",
    "        evaluate(model, valid_in_data, valid_out_data, metrics, 1)[0]\n",
    "    )\n",
    "    \n",
    "    if valid_scores[-1] > best_r10:\n",
    "        best_r10 = valid_scores[-1]\n",
    "        model_best.load_state_dict(deepcopy(model.state_dict()))\n",
    "    \n",
    "    print('=' * 89)\n",
    "    print(f'epoch {epoch} | valid recall@10: {valid_scores[-1]:.4f} | ' +\n",
    "          f'best valid: {best_r10:.4f} | train recall@10: {train_scores[-1]:.4f}')\n",
    "    print('=' * 89)\n",
    "    \n",
    "\n",
    "#test_metrics = [{'metric': Recall_at_k_batch, 'k': 100}, {'metric': Recall_at_k_batch, 'k': 20}, {'metric': Recall_at_k_batch, 'k': 50}]\n",
    "\n",
    "# Run on test data.\n",
    "final_scores = evaluate(model_best, test_in_data, test_out_data, metrics)\n",
    "\n",
    "print('=' * 89)\n",
    "for metric, score in zip(metrics, final_scores):\n",
    "    print(f\"| End of training | {metric['metric'].__name__}@{metric['k']}:\\t{score:.4f}\")\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Î™®Îç∏ Ï†ÄÏû•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, saved_dir):\n",
    "    os.makedirs(saved_dir, exist_ok=True)\n",
    "    file_name=f'recvae_all.pt'\n",
    "    check_point = {\n",
    "        'net': model.state_dict()\n",
    "    }\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(check_point,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, '/opt/ml/input/code/output/models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/opt/ml/input/code/output/models/recvae_all.pt'\n",
    "checkpoint = torch.load(model_path,map_location=device)\n",
    "state_dict = checkpoint['net']\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, sub_data):\n",
    "    model.eval()\n",
    "    pred_list = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in generate(batch_size=args.batch_size, device=device, data_in=sub_data):\n",
    "            #print(batch.get_idx())\n",
    "            \n",
    "            ratings = batch.get_ratings_to_dev()\n",
    "            #print(ratings)\n",
    "            \n",
    "            ratings_pred = model(ratings, calculate_loss=False).cpu().detach().numpy()\n",
    "            #print(ratings_pred)\n",
    "            \n",
    "            ratings_pred[batch.get_ratings().nonzero()] = -np.inf\n",
    "            \n",
    "            for i in range(ratings_pred.shape[0]):\n",
    "                pred_list.append(ratings_pred[i])\n",
    "    return pred_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= inference(model, sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    11,     14,     18,     25,     31,     35,     43,     50,\n",
       "                58,     60,\n",
       "            ...\n",
       "            138459, 138461, 138470, 138471, 138472, 138473, 138475, 138486,\n",
       "            138492, 138493],\n",
       "           dtype='int64', name='user', length=31360)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4643,    170,    531, ...,   6519,   8830, 102880])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31360/31360 [00:16<00:00, 1929.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "result = []\n",
    "\n",
    "for user in tqdm(range(len(all_users))):\n",
    "    ind = np.argsort(preds[user])[::-1]\n",
    "    top_k_items = unique_sid[ind[:10]]\n",
    "    for i in range(10):\n",
    "        result.append((all_users[user], top_k_items[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result, columns=[\"user\", \"item\"]).to_csv(\"/opt/ml/input/code/output/submission_recvae_all_50epoch_500batch_lr5e-4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Mission2_Multi-VAE-Ï†ïÎãµ.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

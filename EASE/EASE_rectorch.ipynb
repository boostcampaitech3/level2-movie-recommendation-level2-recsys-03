{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQG9hL8-UQUP"
   },
   "source": [
    "# EASE\n",
    "üìöReferences  \n",
    "https://makgyver.github.io/rectorch/index.html  \n",
    "https://github.com/makgyver/rectorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7CfnRw7U59C"
   },
   "source": [
    "#### Ï¥àÍ∏∞ ÏÑ∏ÌåÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20039,
     "status": "ok",
     "timestamp": 1647338351940,
     "user": {
      "displayName": "Jungwon Seo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-5uwnfogZleKrsyZupiP0LpS_PBPloa_pqKvqTg=s64",
      "userId": "00987919963196484028"
     },
     "user_tz": -540
    },
    "id": "EWWEf1mPKdnX",
    "outputId": "c2ab2ddf-9a02-4bc8-8133-ff5c95f9e366"
   },
   "outputs": [],
   "source": [
    "# !pip install pandas==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bQj6k1mSbxaz"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1647338362900,
     "user": {
      "displayName": "Jungwon Seo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-5uwnfogZleKrsyZupiP0LpS_PBPloa_pqKvqTg=s64",
      "userId": "00987919963196484028"
     },
     "user_tz": -540
    },
    "id": "xQ3W0udmbxa3",
    "outputId": "a711d6bb-0f42-451f-d97b-7fed34682010"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Í∞ÅÏ¢Ö ÌååÎùºÎØ∏ÌÑ∞ ÏÑ∏ÌåÖ\n",
    "parser = argparse.ArgumentParser(description='rectorch EASE')\n",
    "\n",
    "\n",
    "parser.add_argument('--data', type=str, default='/opt/ml/input/data/train/',\n",
    "                    help='Movielens dataset location')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=2048,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', action='store_true',\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--save', type=str, default='model.pt',\n",
    "                    help='path to save the final model')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# Set the random seed manually for reproductibility.\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "#ÎßåÏïΩ GPUÍ∞Ä ÏÇ¨Ïö©Í∞ÄÎä•Ìïú ÌôòÍ≤ΩÏù¥ÎùºÎ©¥ GPUÎ•º ÏÇ¨Ïö©\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7o1fvXqFWE_G"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cgvNoy1Ybxa6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "\n",
    "    return count\n",
    "\n",
    "# ÌäπÏ†ïÌïú ÌöüÏàò Ïù¥ÏÉÅÏùò Î¶¨Î∑∞Í∞Ä Ï°¥Ïû¨ÌïòÎäî(ÏÇ¨Ïö©ÏûêÏùò Í≤ΩÏö∞ min_uc Ïù¥ÏÉÅ, ÏïÑÏù¥ÌÖúÏùò Í≤ΩÏö∞ min_scÏù¥ÏÉÅ) \n",
    "# Îç∞Ïù¥ÌÑ∞ÎßåÏùÑ Ï∂îÏ∂úÌï† Îïå ÏÇ¨Ïö©ÌïòÎäî Ìï®ÏàòÏûÖÎãàÎã§.\n",
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'item')\n",
    "        tp = tp[tp['item'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'user')\n",
    "        tp = tp[tp['user'].isin(usercount.index[usercount >= min_uc])]\n",
    "\n",
    "    usercount, itemcount = get_count(tp, 'user'), get_count(tp, 'item')\n",
    "    return tp, usercount, itemcount\n",
    "\n",
    "#ÌõàÎ†®Îêú Î™®Îç∏ÏùÑ Ïù¥Ïö©Ìï¥ Í≤ÄÏ¶ùÌï† Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÎ¶¨ÌïòÎäî Ìï®ÏàòÏûÖÎãàÎã§.\n",
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('user')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "    \n",
    "    for _, group in data_grouped_by_user:\n",
    "        n_items_u = len(group)\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        \n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "    \n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    return data_tr, data_te\n",
    "\n",
    "def numerize(tp, profile2id, show2id):\n",
    "    uid = tp['user'].apply(lambda x: profile2id[x])\n",
    "    sid = tp['item'].apply(lambda x: show2id[x])\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2812,
     "status": "ok",
     "timestamp": 1647338365706,
     "user": {
      "displayName": "Jungwon Seo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-5uwnfogZleKrsyZupiP0LpS_PBPloa_pqKvqTg=s64",
      "userId": "00987919963196484028"
     },
     "user_tz": -540
    },
    "id": "fVFoRHrmVQsp",
    "outputId": "d742d219-96a0-4334-afff-685a368ec622"
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "DATA_DIR = args.data\n",
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'train_ratings.csv'), header=0)\n",
    "\n",
    "# Filter Data\n",
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data, min_uc=5, min_sc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1647338365706,
     "user": {
      "displayName": "Jungwon Seo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-5uwnfogZleKrsyZupiP0LpS_PBPloa_pqKvqTg=s64",
      "userId": "00987919963196484028"
     },
     "user_tz": -540
    },
    "id": "7T1dTsWUrffP",
    "outputId": "68476060-3cf6-473e-f21c-05e9cda5bdb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(BEFORE) unique_uid: Int64Index([    11,     14,     18,     25,     31,     35,     43,     50,\n",
      "                58,     60,\n",
      "            ...\n",
      "            138459, 138461, 138470, 138471, 138472, 138473, 138475, 138486,\n",
      "            138492, 138493],\n",
      "           dtype='int64', name='user', length=31360)\n",
      "(AFTER) unique_uid: Int64Index([ 27968,  67764,   2581,  82969, 137831,  48639,  97870,  40424,\n",
      "             46835,  79570,\n",
      "            ...\n",
      "            114284,   9009,  21165,  33920,  22054, 135379, 125855,  41891,\n",
      "             15720,  17029],\n",
      "           dtype='int64', name='user', length=31360)\n",
      "ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïóê ÏÇ¨Ïö©Îê† ÏÇ¨Ïö©Ïûê Ïàò: 31360\n",
      "Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Ïóê ÏÇ¨Ïö©Îê† ÏÇ¨Ïö©Ïûê Ïàò: 3000\n",
      "ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Ïóê ÏÇ¨Ïö©Îê† ÏÇ¨Ïö©Ïûê Ïàò: 3000\n"
     ]
    }
   ],
   "source": [
    "# Shuffle User Indices\n",
    "unique_uid = user_activity.index\n",
    "all_users = unique_uid\n",
    "print(\"(BEFORE) unique_uid:\",unique_uid)\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]\n",
    "print(\"(AFTER) unique_uid:\",unique_uid)\n",
    "\n",
    "n_users = unique_uid.size #31360\n",
    "n_heldout_users = 3000\n",
    "\n",
    "\n",
    "# Split Train/Validation/Test User Indices\n",
    "#tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "tr_users = unique_uid\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]\n",
    "\n",
    "print(\"ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïóê ÏÇ¨Ïö©Îê† ÏÇ¨Ïö©Ïûê Ïàò:\", len(tr_users))\n",
    "print(\"Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Ïóê ÏÇ¨Ïö©Îê† ÏÇ¨Ïö©Ïûê Ïàò:\", len(vd_users))\n",
    "print(\"ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Ïóê ÏÇ¨Ïö©Îê† ÏÇ¨Ïö©Ïûê Ïàò:\", len(te_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30590,
     "status": "ok",
     "timestamp": 1647338396290,
     "user": {
      "displayName": "Jungwon Seo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-5uwnfogZleKrsyZupiP0LpS_PBPloa_pqKvqTg=s64",
      "userId": "00987919963196484028"
     },
     "user_tz": -540
    },
    "id": "3yBsRCRqtPz6",
    "outputId": "c1dcf49a-a33d-482e-f321-db4fb2cacb1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "##ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïóê Ìï¥ÎãπÌïòÎäî ÏïÑÏù¥ÌÖúÎì§\n",
    "train_plays = raw_data.loc[raw_data['user'].isin(tr_users)]\n",
    "\n",
    "##ÏïÑÏù¥ÌÖú ID\n",
    "unique_sid = pd.unique(train_plays['item'])\n",
    "\n",
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
    "\n",
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "\n",
    "if not os.path.exists(pro_dir):\n",
    "    os.makedirs(pro_dir)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)\n",
    "\n",
    "#ValidationÍ≥º TestÏóêÎäî inputÏúºÎ°ú ÏÇ¨Ïö©Îê† tr Îç∞Ïù¥ÌÑ∞ÏôÄ Ï†ïÎãµÏùÑ ÌôïÏù∏ÌïòÍ∏∞ ÏúÑÌïú te Îç∞Ïù¥ÌÑ∞Î°ú Î∂ÑÎ¶¨ÎêòÏóàÏäµÎãàÎã§.\n",
    "vad_plays = raw_data.loc[raw_data['user'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['item'].isin(unique_sid)]\n",
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
    "\n",
    "test_plays = raw_data.loc[raw_data['user'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['item'].isin(unique_sid)]\n",
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
    "'''\n",
    "train_data = numerize(train_plays, profile2id, show2id)\n",
    "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
    "\n",
    "vad_data_tr = numerize(vad_plays_tr, profile2id, show2id)\n",
    "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
    "\n",
    "vad_data_te = numerize(vad_plays_te, profile2id, show2id)\n",
    "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
    "\n",
    "test_data_tr = numerize(test_plays_tr, profile2id, show2id)\n",
    "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
    "\n",
    "test_data_te = numerize(test_plays_te, profile2id, show2id)\n",
    "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)\n",
    "'''\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           uid   sid\n",
      "0            0     0\n",
      "1            0     1\n",
      "2            0     2\n",
      "3            0     3\n",
      "4            0     4\n",
      "...        ...   ...\n",
      "5154466  31359   423\n",
      "5154467  31359  1491\n",
      "5154468  31359   331\n",
      "5154469  31359   733\n",
      "5154470  31359  2256\n",
      "\n",
      "[5154471 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#submissionÏóê ÏÇ¨Ïö©Ìï† Îç∞Ïù¥ÌÑ∞\n",
    "submission_plays = raw_data.loc[raw_data['user'].isin(all_users)]\n",
    "\n",
    "##ÏïÑÏù¥ÌÖú ID\n",
    "unique_sid = pd.unique(submission_plays['item'])\n",
    "\n",
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(all_users))\n",
    "\n",
    "sub_data = numerize(submission_plays, profile2id, show2id)\n",
    "sub_data.to_csv(os.path.join(pro_dir, 'sub.csv'), index=False)\n",
    "print(sub_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMiq9leyWWL1"
   },
   "source": [
    "## Îç∞Ïù¥ÌÑ∞ Î°úÎçî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nxUADr9ibxa8"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DataLoader():\n",
    "    '''\n",
    "    Load Movielens dataset\n",
    "    '''\n",
    "    def __init__(self, path):\n",
    "        \n",
    "        self.pro_dir = os.path.join(path, 'pro_sg')\n",
    "        assert os.path.exists(self.pro_dir), \"Preprocessed files do not exist. Run data.py\"\n",
    "\n",
    "        self.n_items = self.load_n_items()\n",
    "    \n",
    "    def load_data(self, datatype='train'):\n",
    "        if datatype == 'train':\n",
    "            return self._load_train_data()\n",
    "        elif datatype == 'validation':\n",
    "            return self._load_tr_te_data(datatype)\n",
    "        elif datatype == 'test':\n",
    "            return self._load_tr_te_data(datatype)\n",
    "        elif datatype == 'sub':\n",
    "            return self._load_submission_data()\n",
    "        else:\n",
    "            raise ValueError(\"datatype should be in [train, validation, test, sub]\")\n",
    "        \n",
    "    def load_n_items(self):\n",
    "        unique_sid = list()\n",
    "        with open(os.path.join(self.pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                unique_sid.append(line.strip())\n",
    "        n_items = len(unique_sid)\n",
    "        return n_items\n",
    "    \n",
    "    def _load_submission_data(self):\n",
    "        path = os.path.join(self.pro_dir, 'sub.csv')\n",
    "        \n",
    "        tp = pd.read_csv(path)\n",
    "        n_users = tp['uid'].max() + 1\n",
    "        rows, cols = tp['uid'], tp['sid']\n",
    "        data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                                 (rows, cols)), dtype='float64',\n",
    "                                 shape=(n_users, self.n_items))\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _load_train_data(self):\n",
    "        path = os.path.join(self.pro_dir, 'train.csv')\n",
    "        \n",
    "        tp = pd.read_csv(path)\n",
    "        n_users = tp['uid'].max() + 1\n",
    "        rows, cols = tp['uid'], tp['sid']\n",
    "        data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                                 (rows, cols)), dtype='float64',\n",
    "                                 shape=(n_users, self.n_items))\n",
    "        return data\n",
    "    \n",
    "    def _load_tr_te_data(self, datatype='test'):\n",
    "        tr_path = os.path.join(self.pro_dir, '{}_tr.csv'.format(datatype))\n",
    "        te_path = os.path.join(self.pro_dir, '{}_te.csv'.format(datatype))\n",
    "\n",
    "        tp_tr = pd.read_csv(tr_path)\n",
    "        tp_te = pd.read_csv(te_path)\n",
    "\n",
    "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "        rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "        rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "        data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                                    (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
    "        data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                                    (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
    "        return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse2torch_sparse(data):\n",
    "    \"\"\"\n",
    "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
    "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
    "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
    "    \"\"\"\n",
    "    samples = data.shape[0]\n",
    "    features = data.shape[1]\n",
    "    coo_data = data.tocoo()\n",
    "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
    "    row_norms_inv = 1 / np.sqrt(data.sum(1))\n",
    "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
    "    values = np.array([row2val[r] for r in coo_data.row])\n",
    "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
    "    return t\n",
    "\n",
    "def naive_sparse2tensor(data):\n",
    "    return torch.FloatTensor(data.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## EASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rectorch in /opt/conda/lib/python3.8/site-packages (0.0.9b0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from rectorch) (1.7.1)\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.8/site-packages (from rectorch) (2.5.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from rectorch) (1.19.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from rectorch) (1.6.0)\n",
      "Requirement already satisfied: Bottleneck in /opt/conda/lib/python3.8/site-packages (from rectorch) (1.3.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from munch->rectorch) (1.15.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch->rectorch) (3.7.4.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install rectorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rectorch.models as model\n",
    "model = model.EASE(1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EASE(lambda=1000.0000) - not trained yet!"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOsCJbb_X9gl"
   },
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "zxNtit6vbxa-"
   },
   "outputs": [],
   "source": [
    "import bottleneck as bn\n",
    "import numpy as np\n",
    "\n",
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1QjCbMBXw4v"
   },
   "source": [
    "## EASE ÌÖåÏä§Ìä∏ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "78zFFNzgbxa_"
   },
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "loader = DataLoader(args.data)\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "valid_data_tr, valid_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')\n",
    "\n",
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "##inferenceÏóê Ïì∏Í≤É\n",
    "sub_data = loader.load_data('sub')\n",
    "\n",
    "s_N = sub_data.shape[0]\n",
    "s_idxlist = list(range(s_N))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, saved_dir):\n",
    "    os.makedirs(saved_dir, exist_ok=True)\n",
    "    file_name=f'ease.pt'\n",
    "    check_point = {\n",
    "        'net': model.state_dict()\n",
    "    }\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(check_point,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EASE' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/opt/ml/input/code/output/models\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, saved_dir)\u001b[0m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(saved_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mease.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m check_point \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnet\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m()\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(saved_dir, file_name)\n\u001b[1;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(check_point,output_path)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EASE' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "save_model(model, '/opt/ml/input/code/output/models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, sub_data):\n",
    "    model.train(sub_data)\n",
    "    \n",
    "    return model.predict(s_idxlist,sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= inference(model, sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[           -inf,            -inf,            -inf, ...,\n",
       "          5.43685777e-03,  5.37829188e-02,  1.44281641e-02],\n",
       "        [-7.36944309e-02,  1.73713560e-03,  1.91297268e-01, ...,\n",
       "          1.46127846e-04,  8.18273791e-03,  3.68319177e-03],\n",
       "        [ 4.44078861e-04, -5.19917932e-02,  7.66425436e-03, ...,\n",
       "          4.27858851e-05,  8.89692859e-04, -1.47466890e-03],\n",
       "        ...,\n",
       "        [ 1.74077770e-02,  1.77434150e-02,  5.24600656e-02, ...,\n",
       "         -4.99517887e-03, -2.00703701e-03, -6.76523350e-03],\n",
       "        [-1.91775760e-02,  2.06910981e-02,  2.83527005e-03, ...,\n",
       "         -2.24324572e-03, -2.14925338e-03, -5.16240691e-03],\n",
       "        [           -inf,  8.65424791e-02,  2.35361253e-02, ...,\n",
       "          1.32728832e-02,  2.49282942e-02,  1.52772877e-02]]),)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    11,     14,     18,     25,     31,     35,     43,     50,\n",
       "                58,     60,\n",
       "            ...\n",
       "            138459, 138461, 138470, 138471, 138472, 138473, 138475, 138486,\n",
       "            138492, 138493],\n",
       "           dtype='int64', name='user', length=31360)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4643,    170,    531, ...,   6519,   8830, 102880])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31360/31360 [00:16<00:00, 1916.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "result = []\n",
    "\n",
    "for user in tqdm(range(len(all_users))):\n",
    "    ind = np.argsort(preds[0][user])[::-1]\n",
    "    top_k_items = unique_sid[ind[:10]]\n",
    "    for i in range(10):\n",
    "        result.append((all_users[user], top_k_items[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result, columns=[\"user\", \"item\"]).to_csv(\"/opt/ml/input/code/output/submission_EASE_rectorch_lamda_1000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EASE(lambda=1000.0000, model size=(31360, 6807))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Mission2_Multi-VAE-Ï†ïÎãµ.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
